# -*- coding: utf-8 -*-
import os
import re
from typing import Any, Dict, Optional

import google.generativeai as genai


class GenerationError(RuntimeError):
    """Raised when the Gemini client cannot be used."""


def is_gemini_configured() -> bool:
    """Return True when a GOOGLE_API_KEY is available."""
    return bool(os.getenv("GOOGLE_API_KEY"))


try:  # pragma: no cover - support package/script usage
    from .prompt_engineering import DEFAULT_LONG_TOKEN_BUDGET, DEFAULT_SHORT_TOKEN_BUDGET
except ImportError:  # pragma: no cover
    from prompt_engineering import DEFAULT_LONG_TOKEN_BUDGET, DEFAULT_SHORT_TOKEN_BUDGET  # type: ignore


def _setup() -> None:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise GenerationError("Ch∆∞a thi·∫øt l·∫≠p GOOGLE_API_KEY n√™n kh√¥ng th·ªÉ g·ªçi Gemini.")
    try:
        genai.configure(api_key=api_key)
    except Exception as exc:  # pragma: no cover - network/runtime error guard
        raise GenerationError(f"Kh√¥ng c·∫•u h√¨nh ƒë∆∞·ª£c Gemini client ({exc}).") from exc


def _postprocess(ans: str) -> str:
    if not ans:
        return ans
    # c·∫Øt m·ªçi d√≤ng ki·ªÉu "Ngu·ªìn:" / "Source:" n·∫øu model t·ª± sinh
    lines = []
    for ln in ans.splitlines():
        if re.match(r"^\s*(Ngu·ªìn|Source)\s*:.*$", ln, flags=re.I):
            continue
        lines.append(ln)
    txt = "\n".join(lines).strip()
    # ch·ªëng l·∫∑p kh·ªëi tr·∫Øng
    txt = re.sub(r"(?:\n\n)+", "\n\n", txt)
    return txt


def _resolve_generation_config(long_answer: bool, max_tokens: Optional[int]) -> Dict[str, Any]:
    resolved_max = max_tokens
    if resolved_max is None:
        resolved_max = DEFAULT_LONG_TOKEN_BUDGET if long_answer else DEFAULT_SHORT_TOKEN_BUDGET
    try:
        resolved_max = int(resolved_max)
    except Exception as e:
        raise GenerationError(f"max_tokens kh√¥ng h·ª£p l·ªá: {resolved_max!r} ({type(resolved_max).__name__})") from e

    return {
        "temperature": 0.6 if long_answer else 0.55,
        "top_p": 0.9,
        "top_k": 40,
        "max_output_tokens": resolved_max,
    }


def generate_answer_gemini(
    prompt: str,
    model: str = "gemini-2.0-flash",
    long_answer: bool = False,
    max_tokens: Optional[int] = None,
) -> str:
    try:
        _setup()
        generation_config = _resolve_generation_config(long_answer, max_tokens)

        if long_answer:
            prompt = f"""{prompt}

[PHONG C√ÅCH]
- VƒÉn phong ngh·ªã lu·∫≠n m·∫°ch l·∫°c (m·ªü‚Äìth√¢n‚Äìk·∫øt).
- Lu·∫≠n ƒëi·ªÉm ‚Üí d·∫´n ch·ª©ng (tr√≠ch 1‚Äì2 c√¢u th∆° khi ph√π h·ª£p) ‚Üí ph√¢n t√≠ch ‚Üí ti·ªÉu k·∫øt.
- Di·ªÖn ƒë·∫°t m·ªÅm m·∫°i, tr√°nh li·ªát k√™ m√°y m√≥c; ∆∞u ti√™n s·ª± s√°ng r√µ v√† c√¥ ƒë·ªçng.
"""

        gm = genai.GenerativeModel(model_name=model, generation_config=generation_config)

        try:
            res = gm.generate_content(
                prompt,
                safety_settings={
                    "HARM_CATEGORY_HATE_SPEECH": "BLOCK_LOW_AND_ABOVE",
                    "HARM_CATEGORY_HARASSMENT": "BLOCK_LOW_AND_ABOVE",
                    "HARM_CATEGORY_SEXUAL": "BLOCK_MEDIUM_AND_ABOVE",
                    "HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_MEDIUM_AND_ABOVE",
                },
            )
        except TypeError as exc:
            # üëâ th∆∞·ªùng g·∫∑p khi c√≥ bi·∫øn sai ki·ªÉu ·ªü ph√≠a tr√™n (UI truy·ªÅn nh·∫ßm), ho·∫∑c SDK ƒë√≤i ki·ªÉu kh√°c
            raise GenerationError(
                f"TypeError t·ª´ Gemini SDK: {exc}. "
                f"debug types: prompt={type(prompt).__name__}, model={type(model).__name__}, "
                f"long_answer={type(long_answer).__name__}, max_tokens={type(max_tokens).__name__}"
            ) from exc
        except Exception as exc:
            raise GenerationError(f"G·ªçi Gemini th·∫•t b·∫°i ({exc}).") from exc
        ...
    except Exception as exc:
        if isinstance(exc, GenerationError): raise
        raise GenerationError(str(exc)) from exc