# -*- coding: utf-8 -*-
import os
import re
from typing import Any, Dict, Optional

import google.generativeai as genai


class GenerationError(RuntimeError):
    """Raised when the Gemini client cannot be used."""


def is_gemini_configured() -> bool:
    """Return True when a GOOGLE_API_KEY is available."""
    return bool(os.getenv("GOOGLE_API_KEY"))


try:  # pragma: no cover - support package/script usage
    from .prompt_engineering import DEFAULT_LONG_TOKEN_BUDGET, DEFAULT_SHORT_TOKEN_BUDGET
except ImportError:  # pragma: no cover
    from prompt_engineering import DEFAULT_LONG_TOKEN_BUDGET, DEFAULT_SHORT_TOKEN_BUDGET  # type: ignore


def _setup() -> None:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise GenerationError("ChÆ°a thiáº¿t láº­p GOOGLE_API_KEY nÃªn khÃ´ng thá»ƒ gá»i Gemini.")
    try:
        genai.configure(api_key=api_key)
    except Exception as exc:  # pragma: no cover - network/runtime error guard
        raise GenerationError(f"KhÃ´ng cáº¥u hÃ¬nh Ä‘Æ°á»£c Gemini client ({exc}).") from exc


def _postprocess(ans: Optional[str]) -> str:
    if not ans:
        return ""
    # cáº¯t má»i dÃ²ng kiá»ƒu "Nguá»“n:" / "Source:" náº¿u model tá»± sinh
    lines = []
    for ln in ans.splitlines():
        if re.match(r"^\s*(Nguá»“n|Source)\s*:.*$", ln, flags=re.I):
            continue
        lines.append(ln)
    txt = "\n".join(lines).strip()
    # chá»‘ng láº·p khá»‘i tráº¯ng
    txt = re.sub(r"(?:\n\n)+", "\n\n", txt)
    return txt


def _resolve_generation_config(long_answer: bool, max_tokens: Optional[int]) -> Dict[str, Any]:
    resolved_max = max_tokens
    if resolved_max is None:
        resolved_max = DEFAULT_LONG_TOKEN_BUDGET if long_answer else DEFAULT_SHORT_TOKEN_BUDGET
    try:
        resolved_max = int(resolved_max)
    except Exception as e:
        raise GenerationError(f"max_tokens khÃ´ng há»£p lá»‡: {resolved_max!r} ({type(resolved_max).__name__})") from e

    return {
        "temperature": 0.6 if long_answer else 0.55,
        "top_p": 0.9,
        "top_k": 40,
        "max_output_tokens": resolved_max,
    }


def generate_answer_gemini(
    prompt: str,
    model: str = "gemini-2.0-flash",
    long_answer: bool = False,
    max_tokens: Optional[int] = None,
) -> str:
    try:
        _setup()
        generation_config = _resolve_generation_config(long_answer, max_tokens)

        if long_answer:
            prompt = f"""{prompt}

[PHONG CÃCH]
- VÄƒn phong nghá»‹ luáº­n máº¡ch láº¡c (má»Ÿâ€“thÃ¢nâ€“káº¿t).
- Luáº­n Ä‘iá»ƒm â†’ dáº«n chá»©ng (trÃ­ch 1â€“2 cÃ¢u thÆ¡ khi phÃ¹ há»£p) â†’ phÃ¢n tÃ­ch â†’ tiá»ƒu káº¿t.
- Diá»…n Ä‘áº¡t má»m máº¡i, trÃ¡nh liá»‡t kÃª mÃ¡y mÃ³c; Æ°u tiÃªn sá»± sÃ¡ng rÃµ vÃ  cÃ´ Ä‘á»ng.
"""

        gm = genai.GenerativeModel(model_name=model, generation_config=generation_config)

        try:
            res = gm.generate_content(prompt)
        except TypeError as exc:
            # ğŸ‘‰ thÆ°á»ng gáº·p khi cÃ³ biáº¿n sai kiá»ƒu á»Ÿ phÃ­a trÃªn (UI truyá»n nháº§m), hoáº·c SDK Ä‘Ã²i kiá»ƒu khÃ¡c
            raise GenerationError(
                f"TypeError tá»« Gemini SDK: {exc}. "
                f"debug types: prompt={type(prompt).__name__}, model={type(model).__name__}, "
                f"long_answer={type(long_answer).__name__}, max_tokens={type(max_tokens).__name__}"
            ) from exc
        except Exception as exc:
            raise GenerationError(f"Gá»i Gemini tháº¥t báº¡i ({exc}).") from exc
        ...
    except Exception as exc:
        if isinstance(exc, GenerationError): raise
        raise GenerationError(str(exc)) from exc