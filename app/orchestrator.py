# app/orchestrator.py
# -*- coding: utf-8 -*-
from typing import Dict, Any, List, Tuple, Optional
import os

# Báº­t debug (in kÃ¨m má»™t Ã­t metadata khi lá»—i) báº±ng cÃ¡ch Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng: DEBUG_ORCH=1
_DEBUG_ORCH = os.getenv("DEBUG_ORCH", "0") == "1"

# Máº·c Ä‘á»‹nh áº©n nguá»“n; Ä‘áº·t TKC_SHOW_SOURCES=1 Ä‘á»ƒ báº­t láº¡i
_SHOW_SOURCES = os.getenv("TKC_SHOW_SOURCES", "0").strip().lower() in {"1", "true", "yes", "on"}

def _maybe_sources(srcs: Optional[List[str]]) -> List[str]:
    if not _SHOW_SOURCES:
        return []
    return list(srcs or [])

# ==== Heuristics cho close-reading & poem-only ====
_TRICH_DAN_TRIGGER = ["trÃ­ch", "cÃ¢u thÆ¡", "nguyÃªn vÄƒn", "dáº«n", "lá»¥c bÃ¡t", "nhá»‹p", "váº§n", "Ä‘iá»‡p", "Ä‘á»‘i", "Láº§u NgÆ°ng BÃ­ch", "Äoáº¡n trÆ°á»ng"]
_CLOSE_READING_TRIGGER = ["trá»¯ tÃ¬nh ngoáº¡i Ä‘á»", "Ä‘iá»ƒm nhÃ¬n", "áº©n dá»¥", "nhá»‹p Ä‘iá»‡u", "mapping", "báº£n Ä‘á»“ Ã½ niá»‡m", "close reading"]

def _needs_poem_only(q: str) -> bool:
    ql = (q or "").lower()
    return any(t.lower() in ql for t in _TRICH_DAN_TRIGGER)

def _is_close_reading(q: str) -> bool:
    ql = (q or "").lower()
    return any(t.lower() in ql for t in _CLOSE_READING_TRIGGER)

def _norm_key(q: str) -> str:
    return (q or "").strip().lower()

def _make_cache_key(q: str, *, long_answer: bool, intent: str) -> str:
    return f"{_norm_key(q)}|la={int(bool(long_answer))}|intent={intent}"

def _history_to_text(history: Optional[List[Tuple[str, str]]], max_turns: int = 6) -> str:
    if not history:
        return ""
    h = history[-max_turns:]
    lines = []
    for role, txt in h:
        role = "USER" if role == "user" else "ASSISTANT"
        lines.append(f"[{role}]\n{txt}")
    return "\n\n".join(lines)

def _generation_failure_response(
    intent: str,
    reason: str,
    *,
    sources: Optional[List[str]] = None,
) -> Dict[str, Any]:
    detail = (reason or "").strip()
    message = (
        "ðŸ¤– Xin lá»—i, há»‡ thá»‘ng chÆ°a thá»ƒ gá»i mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i. "
        "Vui lÃ²ng kiá»ƒm tra API key vÃ  káº¿t ná»‘i máº¡ng."
    )
    if detail:
        message += f"\n\nChi tiáº¿t ká»¹ thuáº­t: {detail}"
    # nguá»“n luÃ´n rá»—ng náº¿u _SHOW_SOURCES = False
    return {"intent": intent, "answer": message, "sources": _maybe_sources(sources), "error": detail}

def _safe_generate(
    intent: str,
    prompt: str,
    *,
    sources: Optional[List[str]] = None,
    **gen_kwargs: Any,
):
    # Ã©p kiá»ƒu max_tokens
    if "max_tokens" in gen_kwargs and gen_kwargs["max_tokens"] is not None:
        try:
            gen_kwargs["max_tokens"] = int(gen_kwargs["max_tokens"])
        except Exception:
            del gen_kwargs["max_tokens"]

    def _dbg_meta(p: Any) -> Dict[str, Any]:
        try:
            plen = len(p)
        except Exception:
            plen = 0
        try:
            head = (p if isinstance(p, str) else str(p))[:400]
        except Exception:
            head = ""
        return {
            "model": gen_kwargs.get("model"),
            "max_tokens": gen_kwargs.get("max_tokens"),
            "prompt_type": type(p).__name__,
            "prompt_chars": plen,
            "prompt_head": head,
        }

    try:
        from .generation import generate_answer_gemini
        if not isinstance(prompt, str):
            prompt = str(prompt)
        out: str = generate_answer_gemini(prompt, **gen_kwargs)
        if not (out and out.strip()):
            failure = _generation_failure_response(intent, "Model tráº£ vá» ná»™i dung rá»—ng.", sources=sources)
            if _DEBUG_ORCH:
                failure["debug"] = _dbg_meta(prompt)
            return None, failure
        return out, None
    except Exception as exc:
        failure = _generation_failure_response(intent, str(exc), sources=sources)
        if _DEBUG_ORCH:
            failure["debug"] = _dbg_meta(prompt)
        return None, failure

def answer_with_router(
    query: str,
    k: int = 5,
    gemini_model: str = "gemini-2.0-flash",
    history: Optional[List[Tuple[str, str]]] = None,
    long_answer: bool = False,
    max_tokens: Optional[int] = None,
) -> Dict[str, Any]:
    """
    HÃ m Ä‘iá»u phá»‘i chÃ­nh â€” Ä‘Æ°á»£c UI gá»i.
    """
    # Lazy import
    from .router import route_intent, parse_poem_request
    from .rag_pipeline import answer_question
    from .faq import lookup_faq
    from .cache import get_cached, set_cached
    from .poem_tools import poem_ready, get_opening, get_range, get_single, compare_lines
    from .prompt_engineering import (
        DEFAULT_LONG_TOKEN_BUDGET,
        DEFAULT_SHORT_TOKEN_BUDGET,
        build_generic_prompt,
        build_poem_disambiguation_prompt,
        build_smalltalk_prompt,
        build_poem_compare_prompt,
    )
    from .verifier import verify_poem_quotes

    short_history = _history_to_text(history, max_turns=4)
    full_history = _history_to_text(history, max_turns=8)

    if max_tokens is None:
        max_tokens = DEFAULT_LONG_TOKEN_BUDGET if long_answer else DEFAULT_SHORT_TOKEN_BUDGET

    # 1) FAQ
    hit = lookup_faq(query)
    if hit:
        ans = hit["answer"]
        intent = "faq"
        qkey = _make_cache_key(query, long_answer=long_answer, intent=intent)
        set_cached(qkey, ans)
        return {"intent": intent, "answer": ans, "sources": _maybe_sources([])}

    # 2) Route
    intent = route_intent(query)
    qkey = _make_cache_key(query, long_answer=long_answer, intent=intent)

    # 0) Cache sau khi biáº¿t intent
    cached = get_cached(qkey)
    if cached:
        return {"intent": "cache", "answer": cached, "sources": _maybe_sources([])}

    # ---- Small talk
    if intent == "chitchat":
        prompt = build_smalltalk_prompt(query, history_text=short_history)
        ans, failure = _safe_generate(
            intent, prompt, model=gemini_model, long_answer=long_answer, max_tokens=max_tokens
        )
        if failure:
            return failure
        set_cached(qkey, ans or "")
        return {"intent": intent, "answer": ans or "", "sources": _maybe_sources([])}

    # ---- Generic factual
    if intent == "generic":
        prompt = build_generic_prompt(
            query,
            history_text=full_history,
            depth="expanded" if long_answer else "balanced",
        )
        ans, failure = _safe_generate(
            intent, prompt, model=gemini_model, long_answer=long_answer, max_tokens=max_tokens
        )
        if failure:
            return failure
        set_cached(qkey, ans or "")
        return {"intent": intent, "answer": ans or "", "sources": _maybe_sources([])}

    # ---- Poem mode
    if intent == "poem":
        if not poem_ready():
            msg = "Kho thÆ¡ chÆ°a sáºµn sÃ ng (cáº§n data/interim/poem/poem.txt, má»—i cÃ¢u 1 dÃ²ng)."
            set_cached(qkey, msg)
            return {"intent": "poem", "answer": msg, "sources": _maybe_sources([])}

        spec = parse_poem_request(query)
        if spec:
            kind = spec[0]
            if kind == "opening":
                n = max(1, min(int(spec[1]), 1500))
                lines = get_opening(n)
                txt = "\n".join(f"{i + 1:>4}: {ln}" for i, ln in enumerate(lines))
                ans = f"**{n} cÃ¢u Ä‘áº§u Truyá»‡n Kiá»u:**\n\n{txt}"
                set_cached(qkey, ans)
                return {"intent": "poem", "answer": ans, "sources": _maybe_sources([])}

            if kind == "range":
                a, b = int(spec[1]), int(spec[2])
                if a > b:
                    a, b = b, a
                lines = get_range(a, b)
                txt = "\n".join(f"{a + i:>4}: {ln}" for i, ln in enumerate(lines))
                ans = f"**CÃ¡c cÃ¢u {a}â€“{b} trong Truyá»‡n Kiá»u:**\n\n{txt}"
                set_cached(qkey, ans)
                return {"intent": "poem", "answer": ans, "sources": _maybe_sources([])}

            if kind == "single":
                n = int(spec[1])
                ln = get_single(n)
                if ln:
                    ans = f"**CÃ¢u {n} trong Truyá»‡n Kiá»u:**\n\n{n:>4}: {ln}"
                else:
                    ans = f"ChÆ°a tra Ä‘Æ°á»£c cÃ¢u {n} (vÆ°á»£t ngoÃ i sá»‘ dÃ²ng hiá»‡n cÃ³)."
                set_cached(qkey, ans)
                return {"intent": "poem", "answer": ans, "sources": _maybe_sources([])}

            if kind == "compare":
                a, b = int(spec[1]), int(spec[2])
                line_a, line_b = compare_lines(a, b)
                if not line_a or not line_b:
                    ans = "KhÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ so sÃ¡nh hai cÃ¢u Ä‘Æ°á»£c yÃªu cáº§u."
                    set_cached(qkey, ans)
                    return {"intent": "poem", "answer": ans, "sources": _maybe_sources([])}
                prompt = build_poem_compare_prompt(
                    query,
                    line_a=line_a,
                    line_b=line_b,
                    history_text=short_history,
                )
                ans, failure = _safe_generate(
                    "poem",
                    prompt,
                    model=gemini_model,
                    long_answer=long_answer,
                    max_tokens=max_tokens,
                    sources=[f"cÃ¢u {line_a.number}", f"cÃ¢u {line_b.number}"],
                )
                if failure:
                    return failure
                verification = verify_poem_quotes(ans or "")
                set_cached(qkey, ans or "")
                # nguá»“n luÃ´n rá»—ng/áº©n
                return {
                    "intent": "poem",
                    "answer": ans or "",
                    "sources": _maybe_sources([f"cÃ¢u {line_a.number}", f"cÃ¢u {line_b.number}"]),
                    "verification": verification,
                }

        # KhÃ´ng parse Ä‘Æ°á»£c â€” nhá» model há»i láº¡i ngáº¯n
        prompt = build_poem_disambiguation_prompt(query, history_text=short_history)
        ans, failure = _safe_generate(
            "poem", prompt, model=gemini_model, long_answer=long_answer, max_tokens=max_tokens
        )
        if failure:
            return failure
        verification = verify_poem_quotes(ans or "")
        set_cached(qkey, ans or "")
        return {"intent": "poem", "answer": ans or "", "sources": _maybe_sources([]), "verification": verification}

    # ---- Domain â†’ RAG
    poem_only = _needs_poem_only(query)
    close_reading = _is_close_reading(query)

    pack = answer_question(
        query,
        k=k,
        synthesize="single",
        gen_model=gemini_model,
        force_quote=True,
        long_answer=long_answer,
        history_text=full_history,
        max_tokens=max_tokens,
        # Hints cho RAG pipeline
        prefer_poem_source=poem_only,
        top_evidence=6,
        essay_mode=("hsg" if close_reading else None),
    )

    if pack.get("generation_error"):
        return _generation_failure_response("domain", str(pack["generation_error"]))

    ans = pack.get("answer")
    sources = pack.get("sources", [])
    evidence = pack.get("evidence", [])
    verification = verify_poem_quotes(ans or "") if ans else None

    bad_count = 0
    if isinstance(verification, dict):
        bad_count = len(verification.get("invalid_quotes", [])) + len(verification.get("non_exact", []))

    if ans:
        if poem_only and bad_count >= 2:
            ans += (
                "\n\n**LÆ°u Ã½:** PhÃ¡t hiá»‡n vÃ i trÃ­ch dáº«n chÆ°a khá»›p nguyÃªn vÄƒn. "
                "Báº¡n cÃ³ thá»ƒ yÃªu cáº§u: `trÃ­ch cÃ¢u nâ€“m` Ä‘á»ƒ xem báº£n gá»‘c."
            )
        set_cached(qkey, ans or "")
        return {
            "intent": "domain",
            "answer": ans or "",
            "sources": _maybe_sources(sources),  # sáº½ lÃ  [] náº¿u khÃ´ng báº­t TKC_SHOW_SOURCES
            "verification": verification,
            "evidence": evidence,
        }

    # Fallback â€” dÃ¹ng prompt Ä‘Ã£ build (náº¿u cÃ³)
    p = pack.get("prompt", "")
    if not isinstance(p, str):
        p = str(p)
    ans, failure = _safe_generate(
        "domain",
        p,
        model=gemini_model,
        long_answer=long_answer,
        max_tokens=max_tokens,
    )
    if failure:
        return failure
    verification = verify_poem_quotes(ans or "")
    set_cached(qkey, ans or "")
    return {
        "intent": "domain",
        "answer": ans or "",
        "sources": _maybe_sources(pack.get("sources", [])),
        "verification": verification,
    }
