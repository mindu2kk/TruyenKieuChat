# app/orchestrator.py
# -*- coding: utf-8 -*-
from typing import Dict, Any, List, Tuple, Optional

def _norm_key(q: str) -> str:
    return (q or "").strip().lower()

def _history_to_text(history: Optional[List[Tuple[str, str]]], max_turns: int = 6) -> str:
    if not history:
        return ""
    h = history[-max_turns:]
    lines = []
    for role, txt in h:
        role = "USER" if role == "user" else "ASSISTANT"
        lines.append(f"[{role}]\n{txt}")
    return "\n\n".join(lines)

def _generation_failure_response(intent: str, reason: str, *, sources: Optional[List[str]] = None) -> Dict[str, Any]:
    detail = (reason or "").strip()
    message = (
        "ü§ñ Xin l·ªói, h·ªá th·ªëng ch∆∞a th·ªÉ g·ªçi m√¥ h√¨nh Gemini ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi. "
        "Vui l√≤ng ki·ªÉm tra GOOGLE_API_KEY v√† k·∫øt n·ªëi m·∫°ng."
    )
    if detail:
        message += f"\n\nChi ti·∫øt k·ªπ thu·∫≠t: {detail}"
    return {"intent": intent, "answer": message, "sources": sources or [], "error": detail}

def _safe_generate(
    intent: str,
    prompt: str,
    *,
    sources: Optional[List[str]] = None,
    **gen_kwargs: Any,
) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:
    """
    G·ªçi Gemini an to√†n:
    - √©p ki·ªÉu max_tokens -> int (tr√°nh len(int))
    - n·∫øu model tr·∫£ r·ªóng -> quy th√†nh l·ªói ƒë·ªÉ UI kh√¥ng im l·∫∑ng
    - b·∫Øt m·ªçi exception -> tr·∫£ payload l·ªói th·ªëng nh·∫•t
    """
    if "max_tokens" in gen_kwargs and gen_kwargs["max_tokens"] is not None:
        try:
            gen_kwargs["max_tokens"] = int(gen_kwargs["max_tokens"])
        except Exception:
            del gen_kwargs["max_tokens"]

    try:
        from .generation import generate_answer_gemini
        out: str = generate_answer_gemini(prompt, **gen_kwargs)
        if not (out and out.strip()):
            # ‚ùó Quan tr·ªçng: coi output r·ªóng l√† l·ªói c√≥ th√¥ng b√°o r√µ r√†ng
            return None, _generation_failure_response(intent, "Model tr·∫£ v·ªÅ n·ªôi dung r·ªóng.", sources=sources)
        return out, None
    except Exception as exc:
        return None, _generation_failure_response(intent, str(exc), sources=sources)

def answer_with_router(
    query: str,
    k: int = 5,
    gemini_model: str = "gemini-2.0-flash",
    history: Optional[List[Tuple[str, str]]] = None,
    long_answer: bool = False,
    max_tokens: Optional[int] = None,
) -> Dict[str, Any]:

    # ‚¨áÔ∏è Lazy import t·∫•t c·∫£ submodule PH·ª§ THU·ªòC ‚Äî ch·ªâ khi h√†m ƒë∆∞·ª£c g·ªçi
    from .router import route_intent, parse_poem_request
    from .rag_pipeline import answer_question
    from .faq import lookup_faq
    from .cache import get_cached, set_cached
    from .poem_tools import poem_ready, get_opening, get_range, get_single, compare_lines
    from .prompt_engineering import (
        DEFAULT_LONG_TOKEN_BUDGET,
        DEFAULT_SHORT_TOKEN_BUDGET,
        build_generic_prompt,
        build_poem_disambiguation_prompt,
        build_smalltalk_prompt,
        build_poem_compare_prompt,
    )
    from .verifier import verify_poem_quotes

    qkey = _norm_key(query)
    short_history = _history_to_text(history, max_turns=4)
    full_history = _history_to_text(history, max_turns=8)

    if max_tokens is None:
        max_tokens = DEFAULT_LONG_TOKEN_BUDGET if long_answer else DEFAULT_SHORT_TOKEN_BUDGET

    # 0) cache
    cached = get_cached(qkey)
    if cached:
        return {"intent": "cache", "answer": cached, "sources": []}

    # 1) FAQ
    hit = lookup_faq(query)
    if hit:
        ans = hit["answer"]
        set_cached(qkey, ans)
        return {"intent": "faq", "answer": ans, "sources": []}

    # 2) route
    intent = route_intent(query)

    if intent == "chitchat":
        prompt = build_smalltalk_prompt(query, history_text=short_history)
        ans, failure = _safe_generate(
            intent, prompt, model=gemini_model, long_answer=long_answer, max_tokens=max_tokens
        )
        if failure: return failure
        set_cached(qkey, ans or "")
        return {"intent": intent, "answer": ans or "", "sources": []}

    if intent == "generic":
        prompt = build_generic_prompt(query, history_text=full_history, depth="expanded" if long_answer else "balanced")
        ans, failure = _safe_generate(
            intent, prompt, model=gemini_model, long_answer=long_answer, max_tokens=max_tokens
        )
        if failure: return failure
        set_cached(qkey, ans or "")
        return {"intent": intent, "answer": ans or "", "sources": []}

    if intent == "poem":
        if not poem_ready():
            msg = "Kho th∆° ch∆∞a s·∫µn s√†ng (c·∫ßn data/interim/poem/poem.txt, m·ªói c√¢u 1 d√≤ng)."
            set_cached(qkey, msg)
            return {"intent": "poem", "answer": msg, "sources": []}

        spec = parse_poem_request(query)
        if spec:
            kind = spec[0]
            if kind == "opening":
                n = max(1, min(int(spec[1]), 1500))
                lines = get_opening(n)
                txt = "\n".join(f"{i + 1:>4}: {ln}" for i, ln in enumerate(lines))
                ans = f"**{n} c√¢u ƒë·∫ßu Truy·ªán Ki·ªÅu:**\n\n{txt}"
                set_cached(qkey, ans)
                return {"intent": "poem", "answer": ans, "sources": []}
            if kind == "range":
                a, b = int(spec[1]), int(spec[2])
                if a > b: a, b = b, a
                lines = get_range(a, b)
                txt = "\n".join(f"{a + i:>4}: {ln}" for i, ln in enumerate(lines))
                ans = f"**C√°c c√¢u {a}‚Äì{b} trong Truy·ªán Ki·ªÅu:**\n\n{txt}"
                set_cached(qkey, ans)
                return {"intent": "poem", "answer": ans, "sources": []}
            if kind == "single":
                n = int(spec[1])
                ln = get_single(n)
                ans = f"**C√¢u {n} trong Truy·ªán Ki·ªÅu:**\n\n{n:>4}: {ln}" if ln else f"Ch∆∞a tra ƒë∆∞·ª£c c√¢u {n} (v∆∞·ª£t ngo√†i s·ªë d√≤ng hi·ªán c√≥)."
                set_cached(qkey, ans)
                return {"intent": "poem", "answer": ans, "sources": []}
            if kind == "compare":
                a, b = int(spec[1]), int(spec[2])
                line_a, line_b = compare_lines(a, b)
                if not line_a or not line_b:
                    ans = "Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ so s√°nh hai c√¢u ƒë∆∞·ª£c y√™u c·∫ßu."
                    set_cached(qkey, ans)
                    return {"intent": "poem", "answer": ans, "sources": []}
                prompt = build_poem_compare_prompt(query, line_a=line_a, line_b=line_b, history_text=short_history)
                ans, failure = _safe_generate(
                    "poem", prompt, model=gemini_model, long_answer=long_answer, max_tokens=max_tokens,
                    sources=[f"c√¢u {line_a.number}", f"c√¢u {line_b.number}"],
                )
                if failure: return failure
                verification = verify_poem_quotes(ans or "")
                set_cached(qkey, ans or "")
                return {"intent": "poem", "answer": ans or "", "sources": [f"c√¢u {line_a.number}", f"c√¢u {line_b.number}"], "verification": verification}

        prompt = build_poem_disambiguation_prompt(query, history_text=short_history)
        ans, failure = _safe_generate("poem", prompt, model=gemini_model, long_answer=long_answer, max_tokens=max_tokens)
        if failure: return failure
        verification = verify_poem_quotes(ans or "")
        set_cached(qkey, ans or "")
        return {"intent": "poem", "answer": ans or "", "sources": [], "verification": verification}

    # 3) Domain ‚Üí RAG
    pack = answer_question(
        query, k=k, synthesize="single", gen_model=gemini_model,
        force_quote=True, long_answer=long_answer, history_text=full_history, max_tokens=max_tokens,
    )
    if pack.get("generation_error"):
        return _generation_failure_response("domain", str(pack["generation_error"]))

    ans = pack.get("answer")
    if ans:
        verification = verify_poem_quotes(ans or "")
        set_cached(qkey, ans or "")
        return {"intent": "domain", "answer": ans or "", "sources": [], "verification": verification}

    # 4) fallback ‚Äî d√πng prompt ƒë√£ build
    ans, failure = _safe_generate("domain", pack["prompt"], model=gemini_model, long_answer=long_answer, max_tokens=max_tokens)
    if failure: return failure
    verification = verify_poem_quotes(ans or "")
    set_cached(qkey, ans or "")
    return {"intent": "domain", "answer": ans or "", "sources": [], "verification": verification}
